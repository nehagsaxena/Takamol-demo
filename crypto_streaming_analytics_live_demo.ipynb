{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b3121e3-8354-4835-87f5-c5b7b35b36f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Cryptocurrency Streaming Analytics - LIVE DEMO VERSION\n",
    "\n",
    "## Serverless-Compatible Streaming for Live Demonstrations\n",
    "\n",
    "This notebook is optimized for **live demos** on Databricks Serverless clusters.\n",
    "Key features:\n",
    "\n",
    "- Uses `availableNow` trigger (Serverless-compatible)\n",
    "- Processes all available data in one run, then completes\n",
    "- Provides real-time console output during processing\n",
    "- Can be re-run to process newly arrived data\n",
    "- Works on both Serverless and Classic clusters\n",
    "\n",
    "### Demo Instructions:\n",
    "1. Start `crypto_data_producer` notebook first (set `RUN_DURATION_MINUTES = 10` or more)\n",
    "2. Run this notebook - it will process all available data\n",
    "3. Re-run the streaming cell to process new data as it arrives\n",
    "4. Open a Databricks SQL Dashboard to visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80294a01-0be0-4265-bbd6-28ae5f18161b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce14564b-a792-4d99-b131-7aa9b05a41ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\nLIVE DEMO: CRYPTOCURRENCY STREAMING ANALYTICS\n======================================================================\n\nConfiguration:\n  Catalog: takamol_demo\n  Schema: crypto_streaming\n  Landing Path: /Volumes/takamol_demo/crypto_streaming/crypto_landing/trades\n  Checkpoint Base: /Volumes/takamol_demo/crypto_streaming/crypto_landing/checkpoints/live_demo_20260112\n"
     ]
    }
   ],
   "source": [
    "# Configuration - must match producer settings\n",
    "CATALOG = \"takamol_demo\"\n",
    "SCHEMA = \"crypto_streaming\"\n",
    "VOLUME_NAME = \"crypto_landing\"\n",
    "\n",
    "# Paths\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "LANDING_PATH = f\"{VOLUME_PATH}/trades\"\n",
    "\n",
    "# Session ID for unique checkpoints - use fixed ID for demo continuity\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "SESSION_ID = \"live_demo_\" + datetime.now().strftime(\"%Y%m%d\")\n",
    "CHECKPOINT_BASE = f\"{VOLUME_PATH}/checkpoints/{SESSION_ID}\"\n",
    "\n",
    "# Streaming configuration (using default micro-batch for Serverless compatibility)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LIVE DEMO: CRYPTOCURRENCY STREAMING ANALYTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Catalog: {CATALOG}\")\n",
    "print(f\"  Schema: {SCHEMA}\")\n",
    "print(f\"  Landing Path: {LANDING_PATH}\")\n",
    "print(f\"  Checkpoint Base: {CHECKPOINT_BASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcea7de7-0661-4504-8a43-39b91d5b3191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify Data Producer is Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6265bf81-f3c6-4b26-b362-9d63e3eb30b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nDATA AVAILABILITY CHECK\n============================================================\n  Files in landing zone: 83\n\n  Most recent files:\n    trades_20260112_062422_0e2df7f6.json (0.3 KB)\n    trades_20260112_062352_11a1137f.json (0.3 KB)\n    trades_20260112_062345_0e9781bf.json (0.3 KB)\n\n  Status: READY TO STREAM\n"
     ]
    }
   ],
   "source": [
    "# Check if data is available and monitor file count\n",
    "try:\n",
    "    files = dbutils.fs.ls(LANDING_PATH)\n",
    "    file_count = len(files)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DATA AVAILABILITY CHECK\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Files in landing zone: {file_count}\")\n",
    "\n",
    "    if file_count > 0:\n",
    "        files_sorted = sorted(files, key=lambda x: x.name, reverse=True)\n",
    "        print(f\"\\n  Most recent files:\")\n",
    "        for f in files_sorted[:3]:\n",
    "            print(f\"    {f.name} ({f.size / 1024:.1f} KB)\")\n",
    "        print(f\"\\n  Status: READY TO STREAM\")\n",
    "    else:\n",
    "        print(f\"\\n  Status: WAITING FOR DATA\")\n",
    "        print(f\"  Please start the crypto_data_producer notebook first!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n  Error: {e}\")\n",
    "    print(f\"\\n  Please run the crypto_data_producer notebook first to generate data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07982920-1f88-4bcb-9050-b891cbbf6421",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create/Reset Delta Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d205c7fa-19a0-47f6-b359-2e1a9c0ea526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fresh tables for live demo...\nCleared old checkpoints from /Volumes/takamol_demo/crypto_streaming/crypto_landing/checkpoints/live_demo_20260112\n  Created: takamol_demo.crypto_streaming.trades_raw_live\n  Created: takamol_demo.crypto_streaming.trades_analytics_live\n  Created: takamol_demo.crypto_streaming.price_alerts_live\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Save Python's built-in abs before it gets shadowed by PySpark's abs\n",
    "import builtins\n",
    "py_abs = builtins.abs\n",
    "\n",
    "# Ensure schema exists\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# Table names\n",
    "trades_raw_table = f\"{CATALOG}.{SCHEMA}.trades_raw_live\"\n",
    "trades_analytics_table = f\"{CATALOG}.{SCHEMA}.trades_analytics_live\"\n",
    "price_alerts_table = f\"{CATALOG}.{SCHEMA}.price_alerts_live\"\n",
    "\n",
    "# Reset tables for fresh demo (comment out if you want to preserve data)\n",
    "print(\"Creating fresh tables for live demo...\")\n",
    "\n",
    "# Clean up old checkpoints to avoid Delta table ID conflicts\n",
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_BASE, recurse=True)\n",
    "    print(f\"Cleared old checkpoints from {CHECKPOINT_BASE}\")\n",
    "except:\n",
    "    pass  # Checkpoints may not exist yet\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {trades_raw_table}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {trades_raw_table} (\n",
    "        event_type STRING,\n",
    "        event_time BIGINT,\n",
    "        symbol STRING,\n",
    "        trade_id BIGINT,\n",
    "        price DOUBLE,\n",
    "        quantity DOUBLE,\n",
    "        buyer_order_id BIGINT,\n",
    "        seller_order_id BIGINT,\n",
    "        trade_time BIGINT,\n",
    "        is_buyer_maker BOOLEAN,\n",
    "        trade_value_usdt DOUBLE,\n",
    "        ingestion_time BIGINT,\n",
    "        producer_id STRING,\n",
    "        processing_time TIMESTAMP,\n",
    "        trade_timestamp TIMESTAMP,\n",
    "        batch_id BIGINT\n",
    "    )\n",
    "    COMMENT 'Live demo: Raw cryptocurrency trades'\n",
    "\"\"\")\n",
    "print(f\"  Created: {trades_raw_table}\")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {trades_analytics_table}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {trades_analytics_table} (\n",
    "        window_start TIMESTAMP,\n",
    "        window_end TIMESTAMP,\n",
    "        symbol STRING,\n",
    "        trade_count BIGINT,\n",
    "        total_volume DOUBLE,\n",
    "        total_value_usdt DOUBLE,\n",
    "        vwap DOUBLE,\n",
    "        avg_price DOUBLE,\n",
    "        min_price DOUBLE,\n",
    "        max_price DOUBLE,\n",
    "        price_range_pct DOUBLE,\n",
    "        buy_volume DOUBLE,\n",
    "        sell_volume DOUBLE,\n",
    "        buy_sell_ratio DOUBLE,\n",
    "        batch_id BIGINT\n",
    "    )\n",
    "    COMMENT 'Live demo: Trade analytics with VWAP'\n",
    "\"\"\")\n",
    "print(f\"  Created: {trades_analytics_table}\")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {price_alerts_table}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {price_alerts_table} (\n",
    "        alert_time TIMESTAMP,\n",
    "        symbol STRING,\n",
    "        alert_type STRING,\n",
    "        current_price DOUBLE,\n",
    "        previous_price DOUBLE,\n",
    "        price_change_pct DOUBLE,\n",
    "        volume_in_window DOUBLE,\n",
    "        trade_count BIGINT,\n",
    "        severity STRING,\n",
    "        message STRING,\n",
    "        batch_id BIGINT\n",
    "    )\n",
    "    COMMENT 'Live demo: Price movement alerts'\n",
    "\"\"\")\n",
    "print(f\"  Created: {price_alerts_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d30407a-2947-45cf-8daa-9d884042c4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Stream Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4c3b00d-8077-4871-a515-1118d6e9b507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for trade data\n",
    "trade_schema = StructType([\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_time\", LongType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"trade_id\", LongType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", DoubleType(), True),\n",
    "    StructField(\"buyer_order_id\", LongType(), True),\n",
    "    StructField(\"seller_order_id\", LongType(), True),\n",
    "    StructField(\"trade_time\", LongType(), True),\n",
    "    StructField(\"is_buyer_maker\", BooleanType(), True),\n",
    "    StructField(\"trade_value_usdt\", DoubleType(), True),\n",
    "    StructField(\"ingestion_time\", LongType(), True),\n",
    "    StructField(\"producer_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Track statistics globally\n",
    "batch_stats = {\"total_trades\": 0, \"total_batches\": 0, \"alerts_generated\": 0}\n",
    "\n",
    "def process_raw_trades_batch(batch_df, batch_id):\n",
    "    \"\"\"Process and persist raw trade data with live output.\"\"\"\n",
    "    global batch_stats\n",
    "\n",
    "    num_trades = batch_df.count()\n",
    "    if num_trades > 0:\n",
    "        batch_stats[\"total_trades\"] += num_trades\n",
    "        batch_stats[\"total_batches\"] += 1\n",
    "\n",
    "        # Enrich with processing metadata\n",
    "        enriched_df = batch_df \\\n",
    "            .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "            .withColumn(\"trade_timestamp\",\n",
    "                (col(\"trade_time\") / 1000).cast(\"timestamp\")) \\\n",
    "            .withColumn(\"batch_id\", lit(batch_id).cast(\"bigint\"))\n",
    "\n",
    "        # Write to Delta table\n",
    "        enriched_df.write.mode(\"append\").saveAsTable(trades_raw_table)\n",
    "\n",
    "        # Get symbols and prices for display\n",
    "        stats = enriched_df.groupBy(\"symbol\").agg(\n",
    "            count(\"*\").alias(\"trades\"),\n",
    "            round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "            round(sum(\"trade_value_usdt\"), 2).alias(\"volume\")\n",
    "        ).collect()\n",
    "\n",
    "        # Format output for live display\n",
    "        timestamp = datetime.now().strftime('%H:%M:%S')\n",
    "        print(f\"\\n[{timestamp}] Batch {batch_id}: {num_trades} trades ingested\")\n",
    "        for s in stats:\n",
    "            print(f\"  {s.symbol:8} | {s.trades:4} trades | ${s.avg_price:>10,.2f} | Vol: ${s.volume:>12,.2f}\")\n",
    "        print(f\"  Running Total: {batch_stats['total_trades']:,} trades in {batch_stats['total_batches']} batches\")\n",
    "\n",
    "\n",
    "def process_analytics_batch(batch_df, batch_id):\n",
    "    \"\"\"Calculate analytics for each batch.\"\"\"\n",
    "    if batch_df.count() > 0:\n",
    "        analytics_df = batch_df \\\n",
    "            .groupBy(\"symbol\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                sum(\"quantity\").alias(\"total_volume\"),\n",
    "                sum(\"trade_value_usdt\").alias(\"total_value_usdt\"),\n",
    "                (sum(col(\"price\") * col(\"quantity\")) / sum(\"quantity\")).alias(\"vwap\"),\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                min(\"price\").alias(\"min_price\"),\n",
    "                max(\"price\").alias(\"max_price\"),\n",
    "                sum(when(~col(\"is_buyer_maker\"), col(\"quantity\")).otherwise(0)).alias(\"buy_volume\"),\n",
    "                sum(when(col(\"is_buyer_maker\"), col(\"quantity\")).otherwise(0)).alias(\"sell_volume\")\n",
    "            ) \\\n",
    "            .withColumn(\"price_range_pct\",\n",
    "                round((col(\"max_price\") - col(\"min_price\")) / col(\"avg_price\") * 100, 4)) \\\n",
    "            .withColumn(\"buy_sell_ratio\",\n",
    "                round(col(\"buy_volume\") / (col(\"sell_volume\") + 0.0001), 4)) \\\n",
    "            .withColumn(\"window_start\", current_timestamp()) \\\n",
    "            .withColumn(\"window_end\", current_timestamp()) \\\n",
    "            .withColumn(\"batch_id\", lit(batch_id).cast(\"bigint\")) \\\n",
    "            .select(\n",
    "                \"window_start\", \"window_end\", \"symbol\", \"trade_count\",\n",
    "                round(\"total_volume\", 6).alias(\"total_volume\"),\n",
    "                round(\"total_value_usdt\", 2).alias(\"total_value_usdt\"),\n",
    "                round(\"vwap\", 4).alias(\"vwap\"),\n",
    "                round(\"avg_price\", 4).alias(\"avg_price\"),\n",
    "                round(\"min_price\", 4).alias(\"min_price\"),\n",
    "                round(\"max_price\", 4).alias(\"max_price\"),\n",
    "                \"price_range_pct\",\n",
    "                round(\"buy_volume\", 6).alias(\"buy_volume\"),\n",
    "                round(\"sell_volume\", 6).alias(\"sell_volume\"),\n",
    "                \"buy_sell_ratio\",\n",
    "                \"batch_id\"\n",
    "            )\n",
    "\n",
    "        analytics_df.write.mode(\"append\").saveAsTable(trades_analytics_table)\n",
    "\n",
    "\n",
    "# Alert thresholds - sensitive for demo\n",
    "PRICE_CHANGE_THRESHOLD_PCT = 0.1\n",
    "VOLUME_SPIKE_THRESHOLD = 2\n",
    "\n",
    "def process_alerts_batch(batch_df, batch_id):\n",
    "    \"\"\"Detect price movements and volume spikes with live output.\"\"\"\n",
    "    global batch_stats\n",
    "\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "\n",
    "    metrics_df = batch_df \\\n",
    "        .groupBy(\"symbol\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"trade_count\"),\n",
    "            first(\"price\").alias(\"first_price\"),\n",
    "            last(\"price\").alias(\"last_price\"),\n",
    "            min(\"price\").alias(\"min_price\"),\n",
    "            max(\"price\").alias(\"max_price\"),\n",
    "            sum(\"quantity\").alias(\"volume\")\n",
    "        ) \\\n",
    "        .withColumn(\"price_change_pct\",\n",
    "            round((col(\"last_price\") - col(\"first_price\")) / col(\"first_price\") * 100, 4))\n",
    "\n",
    "    alerts = []\n",
    "\n",
    "    for row in metrics_df.collect():\n",
    "        alert = None\n",
    "\n",
    "        if py_abs(row.price_change_pct) > PRICE_CHANGE_THRESHOLD_PCT:\n",
    "            direction = \"UP\" if row.price_change_pct > 0 else \"DOWN\"\n",
    "            severity = \"HIGH\" if py_abs(row.price_change_pct) > 0.5 else \"MEDIUM\"\n",
    "            alert = {\n",
    "                \"alert_time\": datetime.now(),\n",
    "                \"symbol\": row.symbol,\n",
    "                \"alert_type\": f\"PRICE_{direction}\",\n",
    "                \"current_price\": row.last_price,\n",
    "                \"previous_price\": row.first_price,\n",
    "                \"price_change_pct\": row.price_change_pct,\n",
    "                \"volume_in_window\": row.volume,\n",
    "                \"trade_count\": row.trade_count,\n",
    "                \"severity\": severity,\n",
    "                \"message\": f\"{row.symbol} price moved {direction} {py_abs(row.price_change_pct):.2f}% \"\n",
    "                          f\"(${row.first_price:,.2f} -> ${row.last_price:,.2f})\",\n",
    "                \"batch_id\": int(batch_id)\n",
    "            }\n",
    "        elif row.trade_count > VOLUME_SPIKE_THRESHOLD:\n",
    "            alert = {\n",
    "                \"alert_time\": datetime.now(),\n",
    "                \"symbol\": row.symbol,\n",
    "                \"alert_type\": \"VOLUME_SPIKE\",\n",
    "                \"current_price\": row.last_price,\n",
    "                \"previous_price\": row.first_price,\n",
    "                \"price_change_pct\": row.price_change_pct,\n",
    "                \"volume_in_window\": row.volume,\n",
    "                \"trade_count\": row.trade_count,\n",
    "                \"severity\": \"MEDIUM\",\n",
    "                \"message\": f\"{row.symbol} volume spike: {row.trade_count} trades\",\n",
    "                \"batch_id\": int(batch_id)\n",
    "            }\n",
    "\n",
    "        if alert:\n",
    "            alerts.append(alert)\n",
    "            batch_stats[\"alerts_generated\"] += 1\n",
    "            severity_icon = \"!!\" if alert[\"severity\"] == \"HIGH\" else \"!\"\n",
    "            print(f\"\\n  [{severity_icon} ALERT] {alert['message']}\")\n",
    "\n",
    "    if alerts:\n",
    "        alerts_df = spark.createDataFrame(alerts)\n",
    "        alerts_df.write.mode(\"append\").saveAsTable(price_alerts_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e636bf8c-7cb7-4750-af43-00bdf2f02c41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# START STREAMING PROCESSING\n",
    "\n",
    "The cell below starts all three streaming pipelines using `availableNow` trigger.\n",
    "This will process all currently available data and then complete.\n",
    "**Re-run this cell to process new data as it arrives from the producer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f66c08a-e8b6-4b9f-b80f-d0e6e268f40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n======================================================================\nSTARTING STREAMING PIPELINES (SERVERLESS MODE)\n======================================================================\n\nReading from: /Volumes/takamol_demo/crypto_streaming/crypto_landing/trades\nProcessing mode: availableNow (processes all available data, then completes)\nTip: Re-run this cell to process newly arrived data\n----------------------------------------------------------------------\n\nStarting Stream 1: Raw Trades Ingestion...\nStarting Stream 2: Analytics Aggregations...\nStarting Stream 3: Price Alert Monitoring...\n\n======================================================================\nALL STREAMS STARTED - PROCESSING AVAILABLE DATA\n======================================================================\n\nActive streaming queries:\n  - live_alerts: Initializing sources\n\n----------------------------------------------------------------------\nPROCESSING OUTPUT (batches will appear below):\n----------------------------------------------------------------------\n\n======================================================================\nALL STREAMS COMPLETED\n======================================================================\nRe-run this cell to process new data that has arrived.\n"
     ]
    }
   ],
   "source": [
    "# Stop any existing streams first\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping existing stream: {stream.name}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING STREAMING PIPELINES (SERVERLESS MODE)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nReading from: {LANDING_PATH}\")\n",
    "print(f\"Processing mode: availableNow (processes all available data, then completes)\")\n",
    "print(f\"Tip: Re-run this cell to process newly arrived data\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create Auto Loader stream\n",
    "auto_loader_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_BASE}/live_schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .schema(trade_schema)\n",
    "    .load(LANDING_PATH)\n",
    ")\n",
    "\n",
    "# Start Stream 1: Raw Trades Ingestion\n",
    "print(\"\\nStarting Stream 1: Raw Trades Ingestion...\")\n",
    "raw_query = (\n",
    "    auto_loader_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(process_raw_trades_batch)\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/live_raw\")\n",
    "    .trigger(availableNow=True)\n",
    "    .queryName(\"live_raw_trades\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait a moment for first stream to initialize\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "# Start Stream 2: Analytics (reads from Delta)\n",
    "print(\"Starting Stream 2: Analytics Aggregations...\")\n",
    "analytics_stream = spark.readStream.format(\"delta\").table(trades_raw_table)\n",
    "analytics_query = (\n",
    "    analytics_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(process_analytics_batch)\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/live_analytics\")\n",
    "    .trigger(availableNow=True)\n",
    "    .queryName(\"live_analytics\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Start Stream 3: Alerts (reads from Delta)\n",
    "print(\"Starting Stream 3: Price Alert Monitoring...\")\n",
    "alerts_stream = spark.readStream.format(\"delta\").table(trades_raw_table)\n",
    "alerts_query = (\n",
    "    alerts_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(process_alerts_batch)\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/live_alerts\")\n",
    "    .trigger(availableNow=True)\n",
    "    .queryName(\"live_alerts\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL STREAMS STARTED - PROCESSING AVAILABLE DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nActive streaming queries:\")\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"  - {stream.name}: {stream.status['message']}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"PROCESSING OUTPUT (batches will appear below):\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Wait for all streams to complete processing available data\n",
    "try:\n",
    "    raw_query.awaitTermination()\n",
    "    analytics_query.awaitTermination()\n",
    "    alerts_query.awaitTermination()\n",
    "except Exception as e:\n",
    "    print(f\"Stream terminated: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL STREAMS COMPLETED\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Re-run this cell to process new data that has arrived.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bcae5b5-82e1-4828-820d-742f45f5ea19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query Live Results\n",
    "\n",
    "Run these cells while streaming is active to see current data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abffdc78-2bea-4cf2-8db8-74ae8b954fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nLIVE DEMO STATISTICS\n============================================================\n\nData in Delta Tables:\n  Raw Trades: 97\n  Analytics Records: 5\n  Price Alerts: 5\n"
     ]
    }
   ],
   "source": [
    "# Current statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"LIVE DEMO STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "raw_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {trades_raw_table}\").collect()[0]['cnt']\n",
    "analytics_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {trades_analytics_table}\").collect()[0]['cnt']\n",
    "alerts_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {price_alerts_table}\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"\\nData in Delta Tables:\")\n",
    "print(f\"  Raw Trades: {raw_count:,}\")\n",
    "print(f\"  Analytics Records: {analytics_count:,}\")\n",
    "print(f\"  Price Alerts: {alerts_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0d1aaf-499e-4217-a99b-a67448d1a65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>trade_timestamp</th><th>symbol</th><th>price</th><th>quantity</th><th>trade_value_usdt</th><th>side</th></tr></thead><tbody><tr><td>2026-01-12T06:24:20.993Z</td><td>ETHUSDT</td><td>3157.21</td><td>3.0</td><td>9471.630000000001</td><td>SELL</td></tr><tr><td>2026-01-12T06:23:50.447Z</td><td>BTCUSDT</td><td>92192.29</td><td>5.0E-5</td><td>4.6096145</td><td>BUY</td></tr><tr><td>2026-01-12T06:23:43.076Z</td><td>ETHUSDT</td><td>3158.89</td><td>0.0316</td><td>99.820924</td><td>BUY</td></tr><tr><td>2026-01-12T06:06:48.579Z</td><td>SOLUSDT</td><td>142.98</td><td>0.04</td><td>5.7192</td><td>BUY</td></tr><tr><td>2026-01-12T06:06:20.419Z</td><td>SOLUSDT</td><td>142.98</td><td>0.07</td><td>10.0086</td><td>BUY</td></tr><tr><td>2026-01-12T06:06:11.270Z</td><td>SOLUSDT</td><td>142.95</td><td>0.229</td><td>32.735549999999996</td><td>BUY</td></tr><tr><td>2026-01-12T05:54:26.878Z</td><td>BTCUSDT</td><td>91933.94</td><td>2.2E-4</td><td>20.2254668</td><td>SELL</td></tr><tr><td>2026-01-12T05:53:50.676Z</td><td>ETHUSDT</td><td>3156.87</td><td>1.5093</td><td>4764.663891</td><td>SELL</td></tr><tr><td>2026-01-12T05:53:50.676Z</td><td>ETHUSDT</td><td>3156.87</td><td>0.0999</td><td>315.371313</td><td>SELL</td></tr><tr><td>2026-01-12T05:47:58.690Z</td><td>XRPUSDT</td><td>2.0828</td><td>225.0</td><td>468.63000000000005</td><td>SELL</td></tr><tr><td>2026-01-12T05:47:17.175Z</td><td>BTCUSDT</td><td>92248.0</td><td>2.9E-4</td><td>26.751920000000002</td><td>SELL</td></tr><tr><td>2026-01-12T05:47:13.993Z</td><td>XRPUSDT</td><td>2.0824</td><td>14.4</td><td>29.986559999999997</td><td>SELL</td></tr><tr><td>2026-01-12T05:47:12.673Z</td><td>XRPUSDT</td><td>2.0823</td><td>141.0</td><td>293.6043</td><td>SELL</td></tr><tr><td>2026-01-12T05:46:46.583Z</td><td>BTCUSDT</td><td>92249.06</td><td>2.0E-5</td><td>1.8449812</td><td>BUY</td></tr><tr><td>2026-01-12T05:45:40.401Z</td><td>ETHUSDT</td><td>3168.0</td><td>0.0013</td><td>4.118399999999999</td><td>BUY</td></tr><tr><td>2026-01-12T05:45:31.909Z</td><td>SOLUSDT</td><td>142.78</td><td>0.161</td><td>22.98758</td><td>SELL</td></tr><tr><td>2026-01-12T05:45:15.203Z</td><td>BTCUSDT</td><td>92059.23</td><td>1.4E-4</td><td>12.888292199999999</td><td>SELL</td></tr><tr><td>2026-01-12T05:17:57.344Z</td><td>XRPUSDT</td><td>2.0859</td><td>41.5</td><td>86.56485</td><td>SELL</td></tr><tr><td>2026-01-12T05:15:29.012Z</td><td>BNBUSDT</td><td>906.8</td><td>0.011</td><td>9.974799999999998</td><td>SELL</td></tr><tr><td>2026-01-12T05:14:38.569Z</td><td>XRPUSDT</td><td>2.0885</td><td>2.7</td><td>5.6389499999999995</td><td>BUY</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-12T06:24:20.993Z",
         "ETHUSDT",
         3157.21,
         3.0,
         9471.630000000001,
         "SELL"
        ],
        [
         "2026-01-12T06:23:50.447Z",
         "BTCUSDT",
         92192.29,
         5.0E-5,
         4.6096145,
         "BUY"
        ],
        [
         "2026-01-12T06:23:43.076Z",
         "ETHUSDT",
         3158.89,
         0.0316,
         99.820924,
         "BUY"
        ],
        [
         "2026-01-12T06:06:48.579Z",
         "SOLUSDT",
         142.98,
         0.04,
         5.7192,
         "BUY"
        ],
        [
         "2026-01-12T06:06:20.419Z",
         "SOLUSDT",
         142.98,
         0.07,
         10.0086,
         "BUY"
        ],
        [
         "2026-01-12T06:06:11.270Z",
         "SOLUSDT",
         142.95,
         0.229,
         32.735549999999996,
         "BUY"
        ],
        [
         "2026-01-12T05:54:26.878Z",
         "BTCUSDT",
         91933.94,
         2.2E-4,
         20.2254668,
         "SELL"
        ],
        [
         "2026-01-12T05:53:50.676Z",
         "ETHUSDT",
         3156.87,
         1.5093,
         4764.663891,
         "SELL"
        ],
        [
         "2026-01-12T05:53:50.676Z",
         "ETHUSDT",
         3156.87,
         0.0999,
         315.371313,
         "SELL"
        ],
        [
         "2026-01-12T05:47:58.690Z",
         "XRPUSDT",
         2.0828,
         225.0,
         468.63000000000005,
         "SELL"
        ],
        [
         "2026-01-12T05:47:17.175Z",
         "BTCUSDT",
         92248.0,
         2.9E-4,
         26.751920000000002,
         "SELL"
        ],
        [
         "2026-01-12T05:47:13.993Z",
         "XRPUSDT",
         2.0824,
         14.4,
         29.986559999999997,
         "SELL"
        ],
        [
         "2026-01-12T05:47:12.673Z",
         "XRPUSDT",
         2.0823,
         141.0,
         293.6043,
         "SELL"
        ],
        [
         "2026-01-12T05:46:46.583Z",
         "BTCUSDT",
         92249.06,
         2.0E-5,
         1.8449812,
         "BUY"
        ],
        [
         "2026-01-12T05:45:40.401Z",
         "ETHUSDT",
         3168.0,
         0.0013,
         4.118399999999999,
         "BUY"
        ],
        [
         "2026-01-12T05:45:31.909Z",
         "SOLUSDT",
         142.78,
         0.161,
         22.98758,
         "SELL"
        ],
        [
         "2026-01-12T05:45:15.203Z",
         "BTCUSDT",
         92059.23,
         1.4E-4,
         12.888292199999999,
         "SELL"
        ],
        [
         "2026-01-12T05:17:57.344Z",
         "XRPUSDT",
         2.0859,
         41.5,
         86.56485,
         "SELL"
        ],
        [
         "2026-01-12T05:15:29.012Z",
         "BNBUSDT",
         906.8,
         0.011,
         9.974799999999998,
         "SELL"
        ],
        [
         "2026-01-12T05:14:38.569Z",
         "XRPUSDT",
         2.0885,
         2.7,
         5.6389499999999995,
         "BUY"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "trade_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "symbol",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "trade_value_usdt",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "side",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recent trades\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        trade_timestamp,\n",
    "        symbol,\n",
    "        price,\n",
    "        quantity,\n",
    "        trade_value_usdt,\n",
    "        CASE WHEN is_buyer_maker THEN 'SELL' ELSE 'BUY' END as side\n",
    "    FROM {trades_raw_table}\n",
    "    ORDER BY trade_timestamp DESC\n",
    "    LIMIT 20\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7570f782-4044-48ea-81d2-6dfa1a672152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>alert_time</th><th>symbol</th><th>alert_type</th><th>severity</th><th>message</th><th>price_change_pct</th></tr></thead><tbody><tr><td>2026-01-12T06:24:58.692Z</td><td>SOLUSDT</td><td>PRICE_UP</td><td>HIGH</td><td>SOLUSDT price moved UP 5.01% ($136.16 -> $142.98)</td><td>5.0088</td></tr><tr><td>2026-01-12T06:24:58.692Z</td><td>BNBUSDT</td><td>VOLUME_SPIKE</td><td>MEDIUM</td><td>BNBUSDT volume spike: 10 trades</td><td>0.0673</td></tr><tr><td>2026-01-12T06:24:58.692Z</td><td>ETHUSDT</td><td>PRICE_UP</td><td>HIGH</td><td>ETHUSDT price moved UP 1.89% ($3,098.66 -> $3,157.21)</td><td>1.8895</td></tr><tr><td>2026-01-12T06:24:58.692Z</td><td>XRPUSDT</td><td>PRICE_DOWN</td><td>MEDIUM</td><td>XRPUSDT price moved DOWN 0.41% ($2.09 -> $2.08)</td><td>-0.4064</td></tr><tr><td>2026-01-12T06:24:58.692Z</td><td>BTCUSDT</td><td>PRICE_UP</td><td>HIGH</td><td>BTCUSDT price moved UP 1.71% ($90,637.99 -> $92,192.29)</td><td>1.7148</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-12T06:24:58.692Z",
         "SOLUSDT",
         "PRICE_UP",
         "HIGH",
         "SOLUSDT price moved UP 5.01% ($136.16 -> $142.98)",
         5.0088
        ],
        [
         "2026-01-12T06:24:58.692Z",
         "BNBUSDT",
         "VOLUME_SPIKE",
         "MEDIUM",
         "BNBUSDT volume spike: 10 trades",
         0.0673
        ],
        [
         "2026-01-12T06:24:58.692Z",
         "ETHUSDT",
         "PRICE_UP",
         "HIGH",
         "ETHUSDT price moved UP 1.89% ($3,098.66 -> $3,157.21)",
         1.8895
        ],
        [
         "2026-01-12T06:24:58.692Z",
         "XRPUSDT",
         "PRICE_DOWN",
         "MEDIUM",
         "XRPUSDT price moved DOWN 0.41% ($2.09 -> $2.08)",
         -0.4064
        ],
        [
         "2026-01-12T06:24:58.692Z",
         "BTCUSDT",
         "PRICE_UP",
         "HIGH",
         "BTCUSDT price moved UP 1.71% ($90,637.99 -> $92,192.29)",
         1.7148
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "alert_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "symbol",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "alert_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "severity",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "message",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price_change_pct",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recent alerts\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        alert_time,\n",
    "        symbol,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        message,\n",
    "        price_change_pct\n",
    "    FROM {price_alerts_table}\n",
    "    ORDER BY alert_time DESC\n",
    "    LIMIT 20\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07ff34d-f59e-4514-8342-895d7bbde650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>symbol</th><th>total_trades</th><th>total_volume_usdt</th><th>low</th><th>high</th><th>avg_price</th></tr></thead><tbody><tr><td>SOLUSDT</td><td>23</td><td>17564.02</td><td>136.16</td><td>142.98</td><td>140.05</td></tr><tr><td>ETHUSDT</td><td>13</td><td>14779.49</td><td>3097.7</td><td>3168.0</td><td>3137.63</td></tr><tr><td>BTCUSDT</td><td>33</td><td>5805.42</td><td>90637.99</td><td>92254.12</td><td>91261.26</td></tr><tr><td>XRPUSDT</td><td>18</td><td>1079.34</td><td>2.08</td><td>2.09</td><td>2.09</td></tr><tr><td>BNBUSDT</td><td>10</td><td>190.26</td><td>904.84</td><td>908.21</td><td>906.29</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SOLUSDT",
         23,
         17564.02,
         136.16,
         142.98,
         140.05
        ],
        [
         "ETHUSDT",
         13,
         14779.49,
         3097.7,
         3168.0,
         3137.63
        ],
        [
         "BTCUSDT",
         33,
         5805.42,
         90637.99,
         92254.12,
         91261.26
        ],
        [
         "XRPUSDT",
         18,
         1079.34,
         2.08,
         2.09,
         2.09
        ],
        [
         "BNBUSDT",
         10,
         190.26,
         904.84,
         908.21,
         906.29
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "symbol",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_trades",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_volume_usdt",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "low",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "high",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "avg_price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Market summary\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        symbol,\n",
    "        COUNT(*) as total_trades,\n",
    "        ROUND(SUM(trade_value_usdt), 2) as total_volume_usdt,\n",
    "        ROUND(MIN(price), 2) as low,\n",
    "        ROUND(MAX(price), 2) as high,\n",
    "        ROUND(AVG(price), 2) as avg_price\n",
    "    FROM {trades_raw_table}\n",
    "    GROUP BY symbol\n",
    "    ORDER BY total_volume_usdt DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50823890-ab1b-4c20-87e3-41ea09374568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff025e3-3e54-4fc1-a95f-b2d7428ffbba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all streaming queries...\n\nAll streams stopped.\n\n============================================================\nFINAL DEMO STATISTICS\n============================================================\n  Total Trades Processed: 97\n  Analytics Records: 5\n  Alerts Generated: 5\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# Stop all streams\n",
    "print(\"Stopping all streaming queries...\")\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"  Stopping: {stream.name}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"\\nAll streams stopped.\")\n",
    "\n",
    "# Final statistics\n",
    "raw_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {trades_raw_table}\").collect()[0]['cnt']\n",
    "analytics_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {trades_analytics_table}\").collect()[0]['cnt']\n",
    "alerts_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {price_alerts_table}\").collect()[0]['cnt']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL DEMO STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Total Trades Processed: {raw_count:,}\")\n",
    "print(f\"  Analytics Records: {analytics_count:,}\")\n",
    "print(f\"  Alerts Generated: {alerts_count:,}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2332de11-6ac1-49fc-9d4f-e963c0264cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Takamol Demo - Live Streaming Analytics**\n",
    "\n",
    "*For business presentations and live demonstrations*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "crypto_streaming_analytics_live_demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}