{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52c24d63-cd0b-466d-94e4-ebd8f89c119d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Cryptocurrency Streaming Analytics\n",
    "\n",
    "## Processing Live Market Data with Databricks Structured Streaming\n",
    "\n",
    "This notebook demonstrates how to process real-time cryptocurrency trade data using\n",
    "Databricks Auto Loader and Structured Streaming. The data is sourced from Binance's\n",
    "WebSocket API via the companion producer notebook.\n",
    "\n",
    "### What You'll See:\n",
    "1. **Auto Loader Ingestion** - Automatically pick up new JSON files as they arrive\n",
    "2. **Real-Time Analytics** - VWAP, volume, price volatility per symbol\n",
    "3. **Windowed Aggregations** - Time-based metrics (per minute, per 5 minutes)\n",
    "4. **Price Alerts** - Detect significant price movements\n",
    "5. **Delta Lake Integration** - Persist streaming data for historical analysis\n",
    "6. **Live Dashboards** - Visualize market activity in real-time\n",
    "\n",
    "### Prerequisites:\n",
    "- Run the **`crypto_data_producer`** notebook first (or in parallel) to generate data\n",
    "- Data should be landing in the Unity Catalog Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af0be375-187a-44f8-b4dd-cca627daea4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e745db-bb34-42c9-ba13-cfc69005e235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nCRYPTOCURRENCY STREAMING ANALYTICS\n============================================================\n\nConfiguration:\n  Catalog: takamol_demo\n  Schema: crypto_streaming\n  Landing Path: /Volumes/takamol_demo/crypto_streaming/crypto_landing/trades\n  Checkpoint Base: /Volumes/takamol_demo/crypto_streaming/crypto_landing/checkpoints/20260112_054612_841e1cc7\n  Session ID: 20260112_054612_841e1cc7\n"
     ]
    }
   ],
   "source": [
    "# Configuration - must match producer settings\n",
    "CATALOG = \"takamol_demo\"\n",
    "SCHEMA = \"crypto_streaming\"\n",
    "VOLUME_NAME = \"crypto_landing\"\n",
    "\n",
    "# Paths\n",
    "VOLUME_PATH = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME_NAME}\"\n",
    "LANDING_PATH = f\"{VOLUME_PATH}/trades\"\n",
    "\n",
    "# Session ID for unique checkpoints\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "SESSION_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + str(uuid.uuid4())[:8]\n",
    "CHECKPOINT_BASE = f\"{VOLUME_PATH}/checkpoints/{SESSION_ID}\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CRYPTOCURRENCY STREAMING ANALYTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Catalog: {CATALOG}\")\n",
    "print(f\"  Schema: {SCHEMA}\")\n",
    "print(f\"  Landing Path: {LANDING_PATH}\")\n",
    "print(f\"  Checkpoint Base: {CHECKPOINT_BASE}\")\n",
    "print(f\"  Session ID: {SESSION_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebf2e7f-f16e-4631-ac8c-ffd3879bba8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify Data Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e217f37-7b79-4cce-b057-4878c7404787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✓ Found 70 files in landing zone\n\nMost recent files:\n  trades_20260112_054543_b26e5e0f.json (0.3 KB)\n  trades_20260112_054533_0cc90eb7.json (0.3 KB)\n  trades_20260112_054515_e418e430.json (0.3 KB)\n  trades_20260112_051757_503428aa.json (0.3 KB)\n  trades_20260112_051529_270d4bec.json (0.3 KB)\n\nSample schema:\nroot\n |-- buyer_order_id: long (nullable = true)\n |-- event_time: long (nullable = true)\n |-- event_type: string (nullable = true)\n |-- ingestion_time: long (nullable = true)\n |-- is_buyer_maker: boolean (nullable = true)\n |-- price: double (nullable = true)\n |-- producer_id: string (nullable = true)\n |-- quantity: double (nullable = true)\n |-- seller_order_id: long (nullable = true)\n |-- symbol: string (nullable = true)\n |-- trade_id: long (nullable = true)\n |-- trade_time: long (nullable = true)\n |-- trade_value_usdt: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Check if data is available\n",
    "try:\n",
    "    files = dbutils.fs.ls(LANDING_PATH)\n",
    "    print(f\"\\n✓ Found {len(files)} files in landing zone\")\n",
    "\n",
    "    # Show recent files\n",
    "    files_sorted = sorted(files, key=lambda x: x.name, reverse=True)\n",
    "    print(\"\\nMost recent files:\")\n",
    "    for f in files_sorted[:5]:\n",
    "        print(f\"  {f.name} ({f.size / 1024:.1f} KB)\")\n",
    "\n",
    "    # Quick data preview\n",
    "    sample_df = spark.read.json(LANDING_PATH).limit(5)\n",
    "    print(f\"\\nSample schema:\")\n",
    "    sample_df.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ No data found in landing zone: {e}\")\n",
    "    print(f\"\\nPlease run the crypto_data_producer notebook first to generate data.\")\n",
    "    dbutils.notebook.exit(\"No data available - run producer first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e06b7a5-69e4-4147-91a2-d078fd714c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create Delta Tables for Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d9c128-f6ac-424d-8357-42d76ab474da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created: takamol_demo.crypto_streaming.trades_raw\n✓ Created: takamol_demo.crypto_streaming.trades_analytics\n✓ Created: takamol_demo.crypto_streaming.price_alerts\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Save Python's built-in abs before it gets shadowed by PySpark's abs\n",
    "import builtins\n",
    "py_abs = builtins.abs\n",
    "\n",
    "# Ensure schema exists\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")\n",
    "\n",
    "# 1. Raw trades table\n",
    "trades_raw_table = f\"{CATALOG}.{SCHEMA}.trades_raw\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {trades_raw_table}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {trades_raw_table} (\n",
    "        event_type STRING,\n",
    "        event_time BIGINT,\n",
    "        symbol STRING,\n",
    "        trade_id BIGINT,\n",
    "        price DOUBLE,\n",
    "        quantity DOUBLE,\n",
    "        buyer_order_id BIGINT,\n",
    "        seller_order_id BIGINT,\n",
    "        trade_time BIGINT,\n",
    "        is_buyer_maker BOOLEAN,\n",
    "        trade_value_usdt DOUBLE,\n",
    "        ingestion_time BIGINT,\n",
    "        producer_id STRING,\n",
    "        processing_time TIMESTAMP,\n",
    "        trade_timestamp TIMESTAMP,\n",
    "        batch_id BIGINT\n",
    "    )\n",
    "    COMMENT 'Raw cryptocurrency trades from Binance WebSocket'\n",
    "\"\"\")\n",
    "print(f\"✓ Created: {trades_raw_table}\")\n",
    "\n",
    "# 2. Analytics aggregations table\n",
    "trades_analytics_table = f\"{CATALOG}.{SCHEMA}.trades_analytics\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {trades_analytics_table}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {trades_analytics_table} (\n",
    "        window_start TIMESTAMP,\n",
    "        window_end TIMESTAMP,\n",
    "        symbol STRING,\n",
    "        trade_count BIGINT,\n",
    "        total_volume DOUBLE,\n",
    "        total_value_usdt DOUBLE,\n",
    "        vwap DOUBLE,\n",
    "        avg_price DOUBLE,\n",
    "        min_price DOUBLE,\n",
    "        max_price DOUBLE,\n",
    "        price_range_pct DOUBLE,\n",
    "        buy_volume DOUBLE,\n",
    "        sell_volume DOUBLE,\n",
    "        buy_sell_ratio DOUBLE,\n",
    "        batch_id BIGINT\n",
    "    )\n",
    "    COMMENT 'Windowed cryptocurrency trade analytics'\n",
    "\"\"\")\n",
    "print(f\"✓ Created: {trades_analytics_table}\")\n",
    "\n",
    "# 3. Price alerts table\n",
    "price_alerts_table = f\"{CATALOG}.{SCHEMA}.price_alerts\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {price_alerts_table}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {price_alerts_table} (\n",
    "        alert_time TIMESTAMP,\n",
    "        symbol STRING,\n",
    "        alert_type STRING,\n",
    "        current_price DOUBLE,\n",
    "        previous_price DOUBLE,\n",
    "        price_change_pct DOUBLE,\n",
    "        volume_in_window DOUBLE,\n",
    "        trade_count BIGINT,\n",
    "        severity STRING,\n",
    "        message STRING,\n",
    "        batch_id BIGINT\n",
    "    )\n",
    "    COMMENT 'Price movement alerts for cryptocurrency trades'\n",
    "\"\"\")\n",
    "print(f\"✓ Created: {price_alerts_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1ec665-f3b3-4afc-98db-ecf403153276",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 1: Auto Loader - Streaming File Ingestion\n",
    "\n",
    "## Reading New JSON Files as They Arrive\n",
    "\n",
    "Auto Loader (`cloudFiles` format) automatically:\n",
    "- Discovers new files in the landing zone\n",
    "- Tracks which files have been processed\n",
    "- Handles schema evolution\n",
    "- Scales to millions of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d7165cb-7902-475b-adf5-bed17498dcfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1 Configure Auto Loader Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9701241a-a15b-4a35-b699-86aba88c9e20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto Loader Stream Schema:\nroot\n |-- event_type: string (nullable = true)\n |-- event_time: long (nullable = true)\n |-- symbol: string (nullable = true)\n |-- trade_id: long (nullable = true)\n |-- price: double (nullable = true)\n |-- quantity: double (nullable = true)\n |-- buyer_order_id: long (nullable = true)\n |-- seller_order_id: long (nullable = true)\n |-- trade_time: long (nullable = true)\n |-- is_buyer_maker: boolean (nullable = true)\n |-- trade_value_usdt: double (nullable = true)\n |-- ingestion_time: long (nullable = true)\n |-- producer_id: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Define schema for trade data\n",
    "trade_schema = StructType([\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_time\", LongType(), True),\n",
    "    StructField(\"symbol\", StringType(), True),\n",
    "    StructField(\"trade_id\", LongType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", DoubleType(), True),\n",
    "    StructField(\"buyer_order_id\", LongType(), True),\n",
    "    StructField(\"seller_order_id\", LongType(), True),\n",
    "    StructField(\"trade_time\", LongType(), True),\n",
    "    StructField(\"is_buyer_maker\", BooleanType(), True),\n",
    "    StructField(\"trade_value_usdt\", DoubleType(), True),\n",
    "    StructField(\"ingestion_time\", LongType(), True),\n",
    "    StructField(\"producer_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Auto Loader stream\n",
    "auto_loader_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_BASE}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaHints\", \"price DOUBLE, quantity DOUBLE, trade_value_usdt DOUBLE\")\n",
    "    .schema(trade_schema)\n",
    "    .load(LANDING_PATH)\n",
    ")\n",
    "\n",
    "print(\"Auto Loader Stream Schema:\")\n",
    "auto_loader_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936956d5-3a15-4b03-8481-e6e993f5ecb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2 Stream Raw Trades to Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "853e47cb-49b5-4a24-bdda-c4d5cd56eac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\nSTARTING RAW TRADES STREAM\n============================================================\nReading from: /Volumes/takamol_demo/crypto_streaming/crypto_landing/trades\nWriting to: takamol_demo.crypto_streaming.trades_raw\n------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/12 05:46:26 Error occurred in _on_start_callback_serverless: User neha.saxena@novigosolutions.com has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again.\nTraceback (most recent call last):\n  File \"/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py\", line 84, in wrapper\n    return func(*args, **kwargs)\n  File \"/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py\", line 531, in _on_start_callback_serverless\n    raise e\n  File \"/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py\", line 523, in _on_start_callback_serverless\n    listener = _set_streaming_user_and_register_listener(user_id, query._session,\n  File \"/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py\", line 175, in _set_streaming_user_and_register_listener\n    raise RuntimeError(\nRuntimeError: User neha.saxena@novigosolutions.com has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-8535008811241395>, line 37\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWriting to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrades_raw_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m60\u001B[39m)\n",
       "\u001B[1;32m     31\u001B[0m raw_trades_query \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m     32\u001B[0m     auto_loader_stream\n",
       "\u001B[1;32m     33\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n",
       "\u001B[1;32m     34\u001B[0m     \u001B[38;5;241m.\u001B[39mforeachBatch(process_raw_trades_batch)\n",
       "\u001B[1;32m     35\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCHECKPOINT_BASE\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/raw_trades\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     36\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\u001B[0;32m---> 37\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m     38\u001B[0m )\n",
       "\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Wait for completion\u001B[39;00m\n",
       "\u001B[1;32m     41\u001B[0m raw_trades_query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/streaming/readwriter.py:641\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n",
       "\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n",
       "\u001B[1;32m    634\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    639\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    640\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n",
       "\u001B[0;32m--> 641\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_start_internal\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    643\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtableName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    644\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    645\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputMode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputMode\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    646\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartitionBy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    647\u001B[0m \u001B[43m        \u001B[49m\u001B[43mqueryName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueryName\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    648\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/streaming/readwriter.py:621\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m    617\u001B[0m \u001B[38;5;66;03m# BEGIN-EDGE\u001B[39;00m\n",
       "\u001B[1;32m    618\u001B[0m \u001B[38;5;66;03m# WARNING: This is a temporary hack\u001B[39;00m\n",
       "\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# TODO [SC-148434] Remove this.\u001B[39;00m\n",
       "\u001B[1;32m    620\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39m_streaming_start_call_back:\n",
       "\u001B[0;32m--> 621\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_streaming_start_call_back\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    622\u001B[0m \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[1;32m    624\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mHasField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery_started_event_json\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:90\u001B[0m, in \u001B[0;36mlog_errors.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     86\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n",
       "\u001B[1;32m     87\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_formatted_timestamp()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Error occurred in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     88\u001B[0m     file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n",
       "\u001B[1;32m     89\u001B[0m traceback\u001B[38;5;241m.\u001B[39mprint_exc(file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n",
       "\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:84\u001B[0m, in \u001B[0;36mlog_errors.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n",
       "\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n",
       "\u001B[1;32m     83\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 84\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m     86\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\n",
       "\u001B[1;32m     87\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_formatted_timestamp()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Error occurred in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m     88\u001B[0m             file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:531\u001B[0m, in \u001B[0;36mDatabricksNotebookStreamingCallback.get_callback.<locals>._on_start_callback_serverless\u001B[0;34m(query)\u001B[0m\n",
       "\u001B[1;32m    529\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    530\u001B[0m     query\u001B[38;5;241m.\u001B[39mstop()\n",
       "\u001B[0;32m--> 531\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:523\u001B[0m, in \u001B[0;36mDatabricksNotebookStreamingCallback.get_callback.<locals>._on_start_callback_serverless\u001B[0;34m(query)\u001B[0m\n",
       "\u001B[1;32m    521\u001B[0m user_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id_getter()\n",
       "\u001B[1;32m    522\u001B[0m command_job_group \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_id_getter()\n",
       "\u001B[0;32m--> 523\u001B[0m listener \u001B[38;5;241m=\u001B[39m \u001B[43m_set_streaming_user_and_register_listener\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m    524\u001B[0m \u001B[43m                                                     \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_listener\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    525\u001B[0m _add_streaming_query_to_record(query, command_job_group, listener\u001B[38;5;241m.\u001B[39m_active)\n",
       "\u001B[1;32m    526\u001B[0m _send_query_start_to_driver(command_job_group, query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:175\u001B[0m, in \u001B[0;36m_set_streaming_user_and_register_listener\u001B[0;34m(user_id, spark, listener)\u001B[0m\n",
       "\u001B[1;32m    173\u001B[0m     spark\u001B[38;5;241m.\u001B[39mstreams\u001B[38;5;241m.\u001B[39maddListener(listener)\n",
       "\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m streaming_user_and_listener[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m user_id:\n",
       "\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n",
       "\u001B[1;32m    176\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    177\u001B[0m     )\n",
       "\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m listener\n",
       "\n",
       "\u001B[0;31mRuntimeError\u001B[0m: User neha.saxena@novigosolutions.com has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RuntimeError",
        "evalue": "User neha.saxena@novigosolutions.com has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again."
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>RuntimeError</span>: User neha.saxena@novigosolutions.com has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again."
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
        "File \u001B[0;32m<command-8535008811241395>, line 37\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWriting to: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrades_raw_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m60\u001B[39m)\n\u001B[1;32m     31\u001B[0m raw_trades_query \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     32\u001B[0m     auto_loader_stream\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;241m.\u001B[39mforeachBatch(process_raw_trades_batch)\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mCHECKPOINT_BASE\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/raw_trades\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;241m.\u001B[39mtrigger(availableNow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 37\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     38\u001B[0m )\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Wait for completion\u001B[39;00m\n\u001B[1;32m     41\u001B[0m raw_trades_query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/streaming/readwriter.py:641\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart\u001B[39m(\n\u001B[1;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    634\u001B[0m     path: Optional[\u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptionalPrimitiveType\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    640\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m StreamingQuery:\n\u001B[0;32m--> 641\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_start_internal\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    643\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtableName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    644\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    645\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputMode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputMode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    646\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpartitionBy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartitionBy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    647\u001B[0m \u001B[43m        \u001B[49m\u001B[43mqueryName\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mqueryName\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/pyspark/sql/connect/streaming/readwriter.py:621\u001B[0m, in \u001B[0;36mDataStreamWriter._start_internal\u001B[0;34m(self, path, tableName, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m    617\u001B[0m \u001B[38;5;66;03m# BEGIN-EDGE\u001B[39;00m\n\u001B[1;32m    618\u001B[0m \u001B[38;5;66;03m# WARNING: This is a temporary hack\u001B[39;00m\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# TODO [SC-148434] Remove this.\u001B[39;00m\n\u001B[1;32m    620\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39m_streaming_start_call_back:\n\u001B[0;32m--> 621\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_streaming_start_call_back\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[1;32m    624\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m start_result\u001B[38;5;241m.\u001B[39mHasField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery_started_event_json\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:90\u001B[0m, in \u001B[0;36mlog_errors.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_formatted_timestamp()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Error occurred in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     88\u001B[0m     file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n\u001B[1;32m     89\u001B[0m traceback\u001B[38;5;241m.\u001B[39mprint_exc(file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:84\u001B[0m, in \u001B[0;36mlog_errors.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m     83\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 84\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m     87\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_formatted_timestamp()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Error occurred in \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     88\u001B[0m             file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stderr__)\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:531\u001B[0m, in \u001B[0;36mDatabricksNotebookStreamingCallback.get_callback.<locals>._on_start_callback_serverless\u001B[0;34m(query)\u001B[0m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    530\u001B[0m     query\u001B[38;5;241m.\u001B[39mstop()\n\u001B[0;32m--> 531\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:523\u001B[0m, in \u001B[0;36mDatabricksNotebookStreamingCallback.get_callback.<locals>._on_start_callback_serverless\u001B[0;34m(query)\u001B[0m\n\u001B[1;32m    521\u001B[0m user_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id_getter()\n\u001B[1;32m    522\u001B[0m command_job_group \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_id_getter()\n\u001B[0;32m--> 523\u001B[0m listener \u001B[38;5;241m=\u001B[39m \u001B[43m_set_streaming_user_and_register_listener\u001B[49m\u001B[43m(\u001B[49m\u001B[43muser_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_session\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m                                                     \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_listener\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    525\u001B[0m _add_streaming_query_to_record(query, command_job_group, listener\u001B[38;5;241m.\u001B[39m_active)\n\u001B[1;32m    526\u001B[0m _send_query_start_to_driver(command_job_group, query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display)\n",
        "File \u001B[0;32m/databricks/python_shell/dbruntime/databricks_connect_streaming_listener.py:175\u001B[0m, in \u001B[0;36m_set_streaming_user_and_register_listener\u001B[0;34m(user_id, spark, listener)\u001B[0m\n\u001B[1;32m    173\u001B[0m     spark\u001B[38;5;241m.\u001B[39mstreams\u001B[38;5;241m.\u001B[39maddListener(listener)\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m streaming_user_and_listener[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m user_id:\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    176\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUser \u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    177\u001B[0m     )\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m listener\n",
        "\u001B[0;31mRuntimeError\u001B[0m: User neha.saxena@novigosolutions.com has already started Streaming queries in this Notebook. Only one user could start Streaming queries in a Serverless Notebook. If you need to start a Streaming query, detach and reattach the notebook and try again."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define foreachBatch function for raw trades\n",
    "def process_raw_trades_batch(batch_df, batch_id):\n",
    "    \"\"\"Process and persist raw trade data.\"\"\"\n",
    "    if batch_df.count() > 0:\n",
    "        # Enrich with processing metadata\n",
    "        enriched_df = batch_df \\\n",
    "            .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "            .withColumn(\"trade_timestamp\",\n",
    "                (col(\"trade_time\") / 1000).cast(\"timestamp\")) \\\n",
    "            .withColumn(\"batch_id\", lit(batch_id).cast(\"bigint\"))\n",
    "\n",
    "        # Write to Delta table\n",
    "        enriched_df.write.mode(\"append\").saveAsTable(trades_raw_table)\n",
    "\n",
    "        # Statistics for this batch\n",
    "        symbols = enriched_df.select(\"symbol\").distinct().collect()\n",
    "        symbol_list = [r.symbol for r in symbols]\n",
    "\n",
    "        print(f\"  Batch {batch_id:3d}: {batch_df.count():5d} trades | \"\n",
    "              f\"Symbols: {', '.join(sorted(symbol_list))} | \"\n",
    "              f\"{datetime.now().strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Start the raw trades stream\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING RAW TRADES STREAM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Reading from: {LANDING_PATH}\")\n",
    "print(f\"Writing to: {trades_raw_table}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "raw_trades_query = (\n",
    "    auto_loader_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(process_raw_trades_batch)\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/raw_trades\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "raw_trades_query.awaitTermination()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_raw = spark.sql(f\"SELECT COUNT(*) as cnt FROM {trades_raw_table}\").collect()[0]['cnt']\n",
    "print(f\"✓ Raw trades stream complete! Total trades: {total_raw:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c37dff8-4e15-4ae5-a04b-98b15038a6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Trades from takamol_demo.crypto_streaming.trades_raw:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>trade_timestamp</th><th>symbol</th><th>price</th><th>quantity</th><th>trade_value_usdt</th><th>trade_side</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "trade_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "symbol",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "trade_value_usdt",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "trade_side",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display raw trades\n",
    "print(f\"Recent Trades from {trades_raw_table}:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        trade_timestamp,\n",
    "        symbol,\n",
    "        price,\n",
    "        quantity,\n",
    "        trade_value_usdt,\n",
    "        CASE WHEN is_buyer_maker THEN 'SELL' ELSE 'BUY' END as trade_side\n",
    "    FROM {trades_raw_table}\n",
    "    ORDER BY trade_timestamp DESC\n",
    "    LIMIT 50\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa57a843-0ea9-4003-9450-26b715859e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 2: Real-Time Analytics\n",
    "\n",
    "## Windowed Aggregations for Market Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264d0b2d-af22-4523-ae3e-fb279c261340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1 Calculate VWAP and Volume Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "266fdfb4-caf8-4337-923e-eabf92e90445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-read from Delta as a stream for downstream processing\n",
    "trades_delta_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .table(trades_raw_table)\n",
    ")\n",
    "\n",
    "# Define foreachBatch function for analytics\n",
    "def process_analytics_batch(batch_df, batch_id):\n",
    "    \"\"\"Calculate windowed analytics for each batch.\"\"\"\n",
    "    if batch_df.count() > 0:\n",
    "        analytics_df = batch_df \\\n",
    "            .groupBy(\"symbol\") \\\n",
    "            .agg(\n",
    "                count(\"*\").alias(\"trade_count\"),\n",
    "                sum(\"quantity\").alias(\"total_volume\"),\n",
    "                sum(\"trade_value_usdt\").alias(\"total_value_usdt\"),\n",
    "                # VWAP = Sum(Price * Volume) / Sum(Volume)\n",
    "                (sum(col(\"price\") * col(\"quantity\")) / sum(\"quantity\")).alias(\"vwap\"),\n",
    "                avg(\"price\").alias(\"avg_price\"),\n",
    "                min(\"price\").alias(\"min_price\"),\n",
    "                max(\"price\").alias(\"max_price\"),\n",
    "                # Buy volume (when is_buyer_maker = false, it's a buy)\n",
    "                sum(when(~col(\"is_buyer_maker\"), col(\"quantity\")).otherwise(0)).alias(\"buy_volume\"),\n",
    "                # Sell volume\n",
    "                sum(when(col(\"is_buyer_maker\"), col(\"quantity\")).otherwise(0)).alias(\"sell_volume\")\n",
    "            ) \\\n",
    "            .withColumn(\"price_range_pct\",\n",
    "                round((col(\"max_price\") - col(\"min_price\")) / col(\"avg_price\") * 100, 4)) \\\n",
    "            .withColumn(\"buy_sell_ratio\",\n",
    "                round(col(\"buy_volume\") / (col(\"sell_volume\") + 0.0001), 4)) \\\n",
    "            .withColumn(\"window_start\", current_timestamp()) \\\n",
    "            .withColumn(\"window_end\", current_timestamp()) \\\n",
    "            .withColumn(\"batch_id\", lit(batch_id).cast(\"bigint\")) \\\n",
    "            .select(\n",
    "                \"window_start\", \"window_end\", \"symbol\", \"trade_count\",\n",
    "                round(\"total_volume\", 6).alias(\"total_volume\"),\n",
    "                round(\"total_value_usdt\", 2).alias(\"total_value_usdt\"),\n",
    "                round(\"vwap\", 4).alias(\"vwap\"),\n",
    "                round(\"avg_price\", 4).alias(\"avg_price\"),\n",
    "                round(\"min_price\", 4).alias(\"min_price\"),\n",
    "                round(\"max_price\", 4).alias(\"max_price\"),\n",
    "                \"price_range_pct\",\n",
    "                round(\"buy_volume\", 6).alias(\"buy_volume\"),\n",
    "                round(\"sell_volume\", 6).alias(\"sell_volume\"),\n",
    "                \"buy_sell_ratio\",\n",
    "                \"batch_id\"\n",
    "            )\n",
    "\n",
    "        analytics_df.write.mode(\"append\").saveAsTable(trades_analytics_table)\n",
    "\n",
    "        # Print summary\n",
    "        for row in analytics_df.collect():\n",
    "            print(f\"  {row.symbol:8s} | VWAP: ${row.vwap:,.2f} | \"\n",
    "                  f\"Volume: {row.total_volume:,.4f} | \"\n",
    "                  f\"B/S Ratio: {row.buy_sell_ratio:.2f}\")\n",
    "\n",
    "# Start analytics stream\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING ANALYTICS STREAM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Writing to: {trades_analytics_table}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "analytics_query = (\n",
    "    trades_delta_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(process_analytics_batch)\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/analytics\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "analytics_query.awaitTermination()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"✓ Analytics stream complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea6ab470-ccac-4cb6-a8b3-67ab0a57d35d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display analytics results\n",
    "print(f\"Trade Analytics from {trades_analytics_table}:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        symbol,\n",
    "        SUM(trade_count) as total_trades,\n",
    "        ROUND(SUM(total_value_usdt), 2) as total_volume_usdt,\n",
    "        ROUND(AVG(vwap), 2) as avg_vwap,\n",
    "        ROUND(MIN(min_price), 2) as session_low,\n",
    "        ROUND(MAX(max_price), 2) as session_high,\n",
    "        ROUND(AVG(buy_sell_ratio), 2) as avg_buy_sell_ratio\n",
    "    FROM {trades_analytics_table}\n",
    "    GROUP BY symbol\n",
    "    ORDER BY total_volume_usdt DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba7643c-f07f-4343-ae17-64bcffe278a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2 Price Movement Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c7608f-b20e-4f26-8b95-94a91dd006ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Re-read for alerts stream\n",
    "trades_for_alerts = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .table(trades_raw_table)\n",
    ")\n",
    "\n",
    "# Alert thresholds (adjusted for testing/demo with small datasets)\n",
    "PRICE_CHANGE_THRESHOLD_PCT = 0.1  # Alert if price changes > 0.1% in a batch\n",
    "VOLUME_SPIKE_THRESHOLD = 2        # Alert if > 2 trades in a batch\n",
    "\n",
    "def process_alerts_batch(batch_df, batch_id):\n",
    "    \"\"\"Detect price movements and volume spikes.\"\"\"\n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "\n",
    "    # Calculate metrics per symbol\n",
    "    metrics_df = batch_df \\\n",
    "        .groupBy(\"symbol\") \\\n",
    "        .agg(\n",
    "            count(\"*\").alias(\"trade_count\"),\n",
    "            first(\"price\").alias(\"first_price\"),\n",
    "            last(\"price\").alias(\"last_price\"),\n",
    "            min(\"price\").alias(\"min_price\"),\n",
    "            max(\"price\").alias(\"max_price\"),\n",
    "            sum(\"quantity\").alias(\"volume\")\n",
    "        ) \\\n",
    "        .withColumn(\"price_change_pct\",\n",
    "            round((col(\"last_price\") - col(\"first_price\")) / col(\"first_price\") * 100, 4))\n",
    "\n",
    "    # Generate alerts\n",
    "    alerts = []\n",
    "\n",
    "    for row in metrics_df.collect():\n",
    "        alert = None\n",
    "\n",
    "        # Price movement alert\n",
    "        if py_abs(row.price_change_pct) > PRICE_CHANGE_THRESHOLD_PCT:\n",
    "            direction = \"UP\" if row.price_change_pct > 0 else \"DOWN\"\n",
    "            severity = \"HIGH\" if py_abs(row.price_change_pct) > 0.5 else \"MEDIUM\"\n",
    "            alert = {\n",
    "                \"alert_time\": datetime.now(),\n",
    "                \"symbol\": row.symbol,\n",
    "                \"alert_type\": f\"PRICE_{direction}\",\n",
    "                \"current_price\": row.last_price,\n",
    "                \"previous_price\": row.first_price,\n",
    "                \"price_change_pct\": row.price_change_pct,\n",
    "                \"volume_in_window\": row.volume,\n",
    "                \"trade_count\": row.trade_count,\n",
    "                \"severity\": severity,\n",
    "                \"message\": f\"{row.symbol} price moved {direction} {py_abs(row.price_change_pct):.2f}% \"\n",
    "                          f\"(${row.first_price:,.2f} → ${row.last_price:,.2f})\",\n",
    "                \"batch_id\": int(batch_id)\n",
    "            }\n",
    "\n",
    "        # Volume spike alert\n",
    "        elif row.trade_count > VOLUME_SPIKE_THRESHOLD:\n",
    "            alert = {\n",
    "                \"alert_time\": datetime.now(),\n",
    "                \"symbol\": row.symbol,\n",
    "                \"alert_type\": \"VOLUME_SPIKE\",\n",
    "                \"current_price\": row.last_price,\n",
    "                \"previous_price\": row.first_price,\n",
    "                \"price_change_pct\": row.price_change_pct,\n",
    "                \"volume_in_window\": row.volume,\n",
    "                \"trade_count\": row.trade_count,\n",
    "                \"severity\": \"MEDIUM\",\n",
    "                \"message\": f\"{row.symbol} volume spike: {row.trade_count} trades in batch\",\n",
    "                \"batch_id\": int(batch_id)\n",
    "            }\n",
    "\n",
    "        if alert:\n",
    "            alerts.append(alert)\n",
    "            print(f\"  \uD83D\uDEA8 ALERT: {alert['message']}\")\n",
    "\n",
    "    # Write alerts to table\n",
    "    if alerts:\n",
    "        alerts_df = spark.createDataFrame(alerts)\n",
    "        alerts_df.write.mode(\"append\").saveAsTable(price_alerts_table)\n",
    "\n",
    "# Start alerts stream\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING PRICE ALERT MONITORING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Thresholds: Price Change > {PRICE_CHANGE_THRESHOLD_PCT}% | Volume > {VOLUME_SPIKE_THRESHOLD} trades\")\n",
    "print(f\"Writing to: {price_alerts_table}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "alerts_query = (\n",
    "    trades_for_alerts\n",
    "    .writeStream\n",
    "    .foreachBatch(process_alerts_batch)\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/alerts\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "alerts_query.awaitTermination()\n",
    "\n",
    "print(\"-\" * 60)\n",
    "total_alerts = spark.sql(f\"SELECT COUNT(*) as cnt FROM {price_alerts_table}\").collect()[0]['cnt']\n",
    "print(f\"✓ Alert monitoring complete! Total alerts: {total_alerts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a007f98d-70d7-4f9f-a558-b98ad8abc791",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display any alerts\n",
    "print(f\"Price Alerts from {price_alerts_table}:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        alert_time,\n",
    "        symbol,\n",
    "        alert_type,\n",
    "        severity,\n",
    "        message,\n",
    "        current_price,\n",
    "        price_change_pct,\n",
    "        trade_count\n",
    "    FROM {price_alerts_table}\n",
    "    ORDER BY alert_time DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4875c1f6-138e-4948-a1c5-9e21d71d5b0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 3: Market Dashboard Queries\n",
    "\n",
    "## Pre-Built Queries for Dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dfe17fd-08af-4c72-850a-d8015ac16d2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.1 Current Market Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e504ee6e-3056-4d4a-ab8e-74954845d50d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MARKET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "display(spark.sql(f\"\"\"\n",
    "    WITH latest_prices AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            price as current_price,\n",
    "            trade_timestamp,\n",
    "            ROW_NUMBER() OVER (PARTITION BY symbol ORDER BY trade_timestamp DESC) as rn\n",
    "        FROM {trades_raw_table}\n",
    "    ),\n",
    "    symbol_stats AS (\n",
    "        SELECT\n",
    "            symbol,\n",
    "            COUNT(*) as total_trades,\n",
    "            SUM(trade_value_usdt) as total_volume_usdt,\n",
    "            MIN(price) as session_low,\n",
    "            MAX(price) as session_high,\n",
    "            FIRST(price) as open_price,\n",
    "            SUM(CASE WHEN NOT is_buyer_maker THEN quantity ELSE 0 END) as buy_volume,\n",
    "            SUM(CASE WHEN is_buyer_maker THEN quantity ELSE 0 END) as sell_volume\n",
    "        FROM {trades_raw_table}\n",
    "        GROUP BY symbol\n",
    "    )\n",
    "    SELECT\n",
    "        s.symbol,\n",
    "        ROUND(lp.current_price, 2) as current_price,\n",
    "        ROUND((lp.current_price - s.open_price) / s.open_price * 100, 2) as change_pct,\n",
    "        s.total_trades,\n",
    "        ROUND(s.total_volume_usdt, 2) as volume_usdt,\n",
    "        ROUND(s.session_low, 2) as session_low,\n",
    "        ROUND(s.session_high, 2) as session_high,\n",
    "        ROUND(s.buy_volume / NULLIF(s.sell_volume, 0), 2) as buy_sell_ratio\n",
    "    FROM symbol_stats s\n",
    "    JOIN latest_prices lp ON s.symbol = lp.symbol AND lp.rn = 1\n",
    "    ORDER BY s.total_volume_usdt DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d42673f-b10c-421c-a47a-1f610be4863f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2 Trade Volume Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f883f659-1f4f-42f3-9808-3d5d71f0005c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Trade Volume by Symbol (Per Minute):\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        date_trunc('minute', trade_timestamp) as minute,\n",
    "        symbol,\n",
    "        COUNT(*) as trades,\n",
    "        ROUND(SUM(trade_value_usdt), 2) as volume_usdt,\n",
    "        ROUND(AVG(price), 2) as avg_price\n",
    "    FROM {trades_raw_table}\n",
    "    GROUP BY date_trunc('minute', trade_timestamp), symbol\n",
    "    ORDER BY minute DESC, volume_usdt DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e1e5eb6-9681-4900-bd4a-40121f3e02bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3 Buy/Sell Pressure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b86777-383b-4b70-a4fe-dc00f5b0c883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Buy/Sell Pressure by Symbol:\")\n",
    "display(spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "        symbol,\n",
    "        COUNT(*) as total_trades,\n",
    "        SUM(CASE WHEN NOT is_buyer_maker THEN 1 ELSE 0 END) as buy_trades,\n",
    "        SUM(CASE WHEN is_buyer_maker THEN 1 ELSE 0 END) as sell_trades,\n",
    "        ROUND(SUM(CASE WHEN NOT is_buyer_maker THEN trade_value_usdt ELSE 0 END), 2) as buy_volume_usdt,\n",
    "        ROUND(SUM(CASE WHEN is_buyer_maker THEN trade_value_usdt ELSE 0 END), 2) as sell_volume_usdt,\n",
    "        ROUND(\n",
    "            SUM(CASE WHEN NOT is_buyer_maker THEN trade_value_usdt ELSE 0 END) /\n",
    "            NULLIF(SUM(CASE WHEN is_buyer_maker THEN trade_value_usdt ELSE 0 END), 0)\n",
    "        , 2) as buy_sell_ratio\n",
    "    FROM {trades_raw_table}\n",
    "    GROUP BY symbol\n",
    "    ORDER BY total_trades DESC\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554ccfa9-7c70-4009-ae4f-a2520cac1add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 4: Continuous Streaming Mode\n",
    "\n",
    "## For Live Demos - Run Streams Continuously\n",
    "\n",
    "The cells above use `trigger(availableNow=True)` which processes all available data and stops.\n",
    "For a live demo where data is continuously arriving, use `processingTime` trigger instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758476c7-f6df-4ae6-89ac-b3381cf08d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 Continuous Analytics Stream (Optional)\n",
    "\n",
    "Uncomment and run this cell for continuous processing during a live demo.\n",
    "**Note:** Stop any previous streams first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f926c542-1691-479a-b561-036928a0f4fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # UNCOMMENT FOR CONTINUOUS STREAMING\n",
    "#\n",
    "# # Stop any existing streams\n",
    "# for stream in spark.streams.active:\n",
    "#     print(f\"Stopping: {stream.name}\")\n",
    "#     stream.stop()\n",
    "#\n",
    "# # Create continuous Auto Loader stream\n",
    "# continuous_stream = (\n",
    "#     spark.readStream\n",
    "#     .format(\"cloudFiles\")\n",
    "#     .option(\"cloudFiles.format\", \"json\")\n",
    "#     .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_BASE}/continuous_schema\")\n",
    "#     .schema(trade_schema)\n",
    "#     .load(LANDING_PATH)\n",
    "# )\n",
    "#\n",
    "# def process_continuous_batch(batch_df, batch_id):\n",
    "#     \"\"\"Process trades continuously with live output.\"\"\"\n",
    "#     if batch_df.count() > 0:\n",
    "#         # Calculate quick stats\n",
    "#         stats = batch_df.groupBy(\"symbol\").agg(\n",
    "#             count(\"*\").alias(\"trades\"),\n",
    "#             round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "#             round(sum(\"trade_value_usdt\"), 2).alias(\"volume\")\n",
    "#         ).collect()\n",
    "#\n",
    "#         print(f\"\\n[Batch {batch_id}] {datetime.now().strftime('%H:%M:%S')} - {batch_df.count()} trades\")\n",
    "#         for s in stats:\n",
    "#             print(f\"  {s.symbol}: {s.trades} trades @ ${s.avg_price:,.2f} (${s.volume:,.2f} vol)\")\n",
    "#\n",
    "#         # Write to Delta\n",
    "#         enriched = batch_df \\\n",
    "#             .withColumn(\"processing_time\", current_timestamp()) \\\n",
    "#             .withColumn(\"trade_timestamp\", (col(\"trade_time\") / 1000).cast(\"timestamp\")) \\\n",
    "#             .withColumn(\"batch_id\", lit(batch_id).cast(\"bigint\"))\n",
    "#         enriched.write.mode(\"append\").saveAsTable(trades_raw_table)\n",
    "#\n",
    "# # Start continuous stream with 5-second intervals\n",
    "# continuous_query = (\n",
    "#     continuous_stream\n",
    "#     .writeStream\n",
    "#     .foreachBatch(process_continuous_batch)\n",
    "#     .option(\"checkpointLocation\", f\"{CHECKPOINT_BASE}/continuous\")\n",
    "#     .trigger(processingTime=\"5 seconds\")  # Process every 5 seconds\n",
    "#     .start()\n",
    "# )\n",
    "#\n",
    "# print(\"Continuous streaming started!\")\n",
    "# print(\"Press 'Cancel' to stop, or run: continuous_query.stop()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9a31f8-f63e-4d00-b6bc-3d0f54415ecb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "# Part 5: Summary & Key Takeaways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f178ac4-aeac-4855-a60f-20621beed916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Final statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"SESSION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "raw_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {trades_raw_table}\").collect()[0]['cnt']\n",
    "analytics_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {trades_analytics_table}\").collect()[0]['cnt']\n",
    "alerts_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {price_alerts_table}\").collect()[0]['cnt']\n",
    "\n",
    "print(f\"\\nData Collected:\")\n",
    "print(f\"  Raw Trades: {raw_count:,}\")\n",
    "print(f\"  Analytics Records: {analytics_count:,}\")\n",
    "print(f\"  Price Alerts: {alerts_count:,}\")\n",
    "\n",
    "print(f\"\\nDelta Tables Created:\")\n",
    "print(f\"  {trades_raw_table}\")\n",
    "print(f\"  {trades_analytics_table}\")\n",
    "print(f\"  {price_alerts_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88e18aa8-411e-4ad5-ba93-d10bf9ef54a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Capabilities Demonstrated\n",
    "\n",
    "| Capability | Description |\n",
    "|------------|-------------|\n",
    "| **Auto Loader** | Automatic file discovery and schema inference |\n",
    "| **Real External Data** | Live cryptocurrency trades from Binance |\n",
    "| **foreachBatch Processing** | Serverless-compatible streaming pattern |\n",
    "| **VWAP Calculation** | Volume-weighted average price |\n",
    "| **Price Alerts** | Real-time anomaly detection |\n",
    "| **Delta Lake** | ACID transactions for streaming data |\n",
    "| **Multi-Stream Architecture** | Separate ingestion from processing |\n",
    "\n",
    "## Production Recommendations\n",
    "\n",
    "1. **Run Producer as Job** - Continuous data ingestion with monitoring\n",
    "2. **Use Delta Live Tables** - Simplified pipeline management\n",
    "3. **Add Schema Registry** - Handle schema evolution gracefully\n",
    "4. **Set Up Alerts** - Route price alerts to Slack/Teams/PagerDuty\n",
    "5. **Optimize Checkpointing** - Use appropriate checkpoint intervals\n",
    "6. **Monitor Latency** - Track end-to-end streaming latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27753f62-859e-47c7-9fdf-5105a2095692",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8047917d-9ca8-477c-8da9-02e61f4ea610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop any running streams\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Stopping stream: {stream.name}\")\n",
    "    stream.stop()\n",
    "\n",
    "print(\"\\n✓ All streams stopped\")\n",
    "\n",
    "# Optional: Clean up checkpoints\n",
    "# dbutils.fs.rm(CHECKPOINT_BASE, recurse=True)\n",
    "# print(\"✓ Checkpoints cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3979f577-3d65-42c1-95c4-158790d7f585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "**Takamol Demo - Real-Time Cryptocurrency Streaming Analytics**\n",
    "\n",
    "*Demonstrating Databricks Structured Streaming with Live Market Data*"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "crypto_streaming_analytics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}